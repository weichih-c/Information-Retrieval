{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run TextPRocessor.ipynb\n",
    "\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputPath = \"\"\n",
    "outputFile = \"Output/classifyResult.txt\"\n",
    "docDirPath = \"IRTM/\"\n",
    "trainingDataLabel = \"Data/training.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractVocabularyFromDocs(docList):\n",
    "    tfPool = dict() # key=Term; value=sum term freq. of all docs in docList\n",
    "    for doc in docList:\n",
    "        termDict = getTermsDictFromDoc(doc) # key=Term; value=tf\n",
    "        for term, tf in termDict.items(): # update tfPool\n",
    "            if term in tfPool.keys():\n",
    "                tfPool[term] += tf\n",
    "            else:\n",
    "                tfPool[term] = tf\n",
    "    return tfPool\n",
    "\n",
    "def extractVocabularyFromSingleDoc(doc):\n",
    "    return getTermsDictFromDoc(doc) # key=Term; value=tf\n",
    "\n",
    "def getTotalVocabularyCountFromDocs(termFreqOfDocs_dict):\n",
    "    return sum(termFreqOfDocs_dict.values())\n",
    "\n",
    "# get each class docs' count and docs' list\n",
    "def readTrainingDataInfos(labeledDocListPath):\n",
    "    classCountDict = dict()\n",
    "    classDocsDict = dict()\n",
    "    with open(labeledDocListPath, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip()\n",
    "            items = line.split(' ')\n",
    "            classID = int(items[0]) # the class_id\n",
    "            countNumber = len(items)-1 # how many docs is that class\n",
    "            classCountDict[classID] = countNumber # count class docs\n",
    "            classDocsDict[classID] = items[1::] # what docs belongs to class\n",
    "    return classCountDict, classDocsDict\n",
    "\n",
    "def countAllDocs(classCountDict):\n",
    "    return sum(classCountDict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return a list of doc path\n",
    "def getClassDocPaths(classDocs, docDirPath):\n",
    "    return [docDirPath + str(doc)+'.txt' for doc in classDocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classCountDict, classDocsDict = readTrainingDataInfos(trainingDataLabel)\n",
    "totalDocsCount = countAllDocs(classCountDict) # count documents numbers (of training set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "Using Chi-Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run FeatureSelection.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionarySet = set() # dictionary of corpus\n",
    "classTerms_dict = {i:[] for i in classDocsDict.keys()} # k=classID, value=list of tuple(docID, termList)\n",
    "\n",
    "r = 0\n",
    "for classID, docList in classDocsDict.items():\n",
    "    for docID in docList:\n",
    "        r+=1\n",
    "        docPath = docDirPath + str(docID) + '.txt'\n",
    "        termList = list(extractVocabularyFromSingleDoc(docPath).keys())\n",
    "        dictionarySet.update(termList)\n",
    "        classTerms_dict[classID].append((docID, termList))\n",
    "classTermChiSquare_dict = {classID: [] for classID in classTerms_dict.keys()}\n",
    "\n",
    "specialTerm = set() # some term might only exist in ontopic (small corpus)\n",
    "\n",
    "\n",
    "for term in dictionarySet:\n",
    "    \n",
    "    for i in classTerms_dict.keys(): # how many classes\n",
    "        termTable = [[0,0],[0,0]] # initial term table for each classID\n",
    "        \n",
    "        for classID, docTermTupleList in classTerms_dict.items(): # for every classID, iterate every class\n",
    "            if i is classID: # on topic\n",
    "                for docTermTuple in docTermTupleList: # count doc number in on topic class\n",
    "                    if term in docTermTuple[1]: # present\n",
    "                        termTable[0][0] += 1\n",
    "                    else:                    # absent\n",
    "                        termTable[0][1] += 1\n",
    "                        \n",
    "            else: # off topic\n",
    "                for docTermTuple in docTermTupleList: # count doc number in off topic class\n",
    "                    if term in docTermTuple[1]: # present\n",
    "                        termTable[1][0] += 1\n",
    "                    else:                    # absent\n",
    "                        termTable[1][1] += 1\n",
    "                        \n",
    "        if(termTable[0][1]==0 and termTable[1][1]==0):\n",
    "            specialTerm.add(term)\n",
    "            continue\n",
    "        chisquareVal = getChiSquare(termTable)\n",
    "        if(chisquareVal[1] == True): # positive indicator of that class\n",
    "            classTermChiSquare_dict[i].append((term, chisquareVal[0])) # append term and score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "print(specialTerm) # check any specialTerm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classTermChiSquare_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# top = list()\n",
    "significantTerms = set()\n",
    "s = 0 \n",
    "for classID, chisquareList in classTermChiSquare_dict.items():\n",
    "    termRankList = sorted(chisquareList, key=lambda x:x[1], reverse = True)\n",
    "    tList = [ rank[0] for rank in termRankList if rank[1] > 49 ]\n",
    "    \n",
    "    if len(tList) > 50: tList = tList[0:50]\n",
    "#     print(\"CLASS-\",classID, \" term count:\", len(tList))\n",
    "    significantTerms.update(set(tList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(len(significantTerms))\n",
    "# print(significantTerms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terms after feature selection: 500\n"
     ]
    }
   ],
   "source": [
    "freakTerms = {'6-1', '6-2', '6-3', '6-4', '7-5', '7-6', 'el', 'pa', 'ho'}\n",
    "significantTerms = significantTerms - freakTerms # after minus freak terms, residual 500 terms.\n",
    "print(\"Terms after feature selection:\", len(significantTerms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier Training \n",
    "Calculate prior probability and condition probability for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run NaiveBayesClassifier.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbClassifier = NaiveBayesClassifier() # initialize classifier\n",
    "\n",
    "for classID in classDocsDict.keys(): # iterate each classID\n",
    "    priorProb = classCountDict[classID] / totalDocsCount # count prior prob of Naive Bayes\n",
    "    classDocs = classDocsDict[classID] # get docs list of classID\n",
    "    docPaths = getClassDocPaths(classDocs, docDirPath)\n",
    "    \n",
    "    # get term and term occur freq from docs in class. (k=term, v=freq)\n",
    "    termFreqOfDocs_dict = extractVocabularyFromDocs(docPaths)\n",
    "    for k in list(termFreqOfDocs_dict.keys()):\n",
    "        if k not in significantTerms:\n",
    "            del termFreqOfDocs_dict[k]\n",
    "    \n",
    "    totalTokenCount = getTotalVocabularyCountFromDocs(termFreqOfDocs_dict) # how many tokens in class docs\n",
    "    smoothTermCount = len(significantTerms) # how many different terms in selected features\n",
    "\n",
    "    condProb = dict()\n",
    "    for term, termOccurFreq in termFreqOfDocs_dict.items(): # get term occur freq (have been calculated)\n",
    "          condProb[term] = (termOccurFreq + 1) / ( totalTokenCount + smoothTermCount)\n",
    "    condProb['OtherTermNotInTrainingData'] = 1 / ( totalTokenCount + smoothTermCount)\n",
    "    \n",
    "    nbClassifier.updateClassPriorProbability(classID, priorProb) # update class prior prob\n",
    "    nbClassifier.updateClassConditionProbability(classID, condProb) # update class cond prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getLogScore(prob):\n",
    "#     print(prob)\n",
    "    return math.log(prob)\n",
    "\n",
    "def addScoreWithFrequency(scoreBuff, score, freq):\n",
    "    for i in range(freq):\n",
    "        scoreBuff += score\n",
    "    return scoreBuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateClassScoreRecord(classScoreList, classID, score):\n",
    "    classScoreList.append((classID, score))\n",
    "        \n",
    "def getResultClass(classScoreList):\n",
    "    classScoreList.sort(key=lambda s:s[1], reverse=True) # Ranking class score from big to small\n",
    "    maxRecord = classScoreList[0] # result is the highest-scored class\n",
    "    resultClass = maxRecord[0] # index0 is class ; 1 is score\n",
    "    return resultClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list all files in given directory\n",
    "def getTestingDocPaths(trainindDataList, docDirPath):\n",
    "    trainingSet = [ train + \".txt\" for train in trainindDataList ]\n",
    "    docList = os.listdir(docDirPath)\n",
    "    return [docDirPath + d for d in docList if d not in trainingSet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainingDataList = []\n",
    "\n",
    "for docList in classDocsDict.values():\n",
    "    trainingDataList.extend(docList)\n",
    "testSet = getTestingDocPaths(trainingDataList, docDirPath)\n",
    "resultList = []\n",
    "for docPath in testSet:\n",
    "    \n",
    "    # macOS filesystem\n",
    "    if docPath == 'Data/test/.DS_Store' or docPath == 'Data/test/.ipynb_checkpoints':continue\n",
    "        \n",
    "    docTermFreqDict = extractVocabularyFromSingleDoc(docPath) # get vocabulary from testing doc\n",
    "    classes = nbClassifier.getAllClass() # get classes in classifier\n",
    "    classScoreList = list()\n",
    "\n",
    "    for c in classes:\n",
    "        scoreOfClass = 0\n",
    "        priorProb = nbClassifier.getClassPriorProbability(c)\n",
    "        priorScore = getLogScore(priorProb) # get prior prob log score\n",
    "        scoreOfClass = addScoreWithFrequency(scoreOfClass, priorScore, 1)\n",
    "\n",
    "        condProb_dict = nbClassifier.getClassConditionProbability(c)\n",
    "        for term, occurFreq in docTermFreqDict.items():\n",
    "            if term not in condProb_dict.keys():\n",
    "                condProb = condProb_dict['OtherTermNotInTrainingData']\n",
    "            else:\n",
    "                condProb = condProb_dict[term]\n",
    "\n",
    "            condProbScore = getLogScore(condProb) # get conditional prob log score\n",
    "            scoreOfClass = addScoreWithFrequency(scoreOfClass, condProbScore, occurFreq)\n",
    "\n",
    "        updateClassScoreRecord(classScoreList, c, scoreOfClass) # update each class's score\n",
    "    resultList.append((docPath.split(docDirPath)[1][:-4], getResultClass(classScoreList)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Classify Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resultList.sort(key=lambda x:int(x[0])) # sort by docID\n",
    "with open(outputFile, 'w') as handle: # output as txt file\n",
    "    for r in resultList:\n",
    "        handle.write(\"%-5s %15s\\n\" % (r[0],r[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

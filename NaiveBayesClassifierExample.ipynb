{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run TextPRocessor.ipynb\n",
    "\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# outputPath = \"\"\n",
    "outputFile = \".txt\"\n",
    "docDirPath = \"IRTM/\"\n",
    "trainingDataLabel = \"Data/training.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list all files in given directory\n",
    "def listFiles(dir_path):\n",
    "    fList = []\n",
    "    for dirPath, dirName, fileNames in os.walk(dir_path):\n",
    "        fList.extend(fileNames)\n",
    "    return fList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractVocabularyFromDocs(docList):\n",
    "    tfPool = dict() # key=Term; value=sum term freq. of all docs in docList\n",
    "    for doc in docList:\n",
    "        termDict = getTermsDictFromDoc(doc) # key=Term; value=tf\n",
    "        for term, tf in termDict.items(): # update tfPool\n",
    "            if term in tfPool.keys():\n",
    "                tfPool[term] += tf\n",
    "            else:\n",
    "                tfPool[term] = tf\n",
    "    return tfPool\n",
    "\n",
    "def extractVocabularyFromSingleDoc(doc):\n",
    "    return getTermsDictFromDoc(doc) # key=Term; value=tf\n",
    "\n",
    "def getTotalVocabularyCountFromDocs(termFreqOfDocs_dict):\n",
    "    return sum(termFreqOfDocs_dict.values())\n",
    "\n",
    "\n",
    "def countDocsInClass(labeledDocListPath):\n",
    "    classCountDict = dict()\n",
    "    classDocsDict = dict()\n",
    "    with open(labeledDocListPath, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip()\n",
    "            items = line.split(' ')\n",
    "            classID = int(items[0]) # the class_id\n",
    "            countNumber = len(items)-1 # how many docs is that class\n",
    "            classCountDict[classID] = countNumber # count class docs\n",
    "            classDocsDict[classID] = items[1::] # what docs belongs to class\n",
    "            \n",
    "    return classCountDict, classDocsDict\n",
    "\n",
    "def countAllDocs(classCountDict):\n",
    "    return sum(classCountDict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return a list of doc path\n",
    "def getClassDocPaths(classDocs, docDirPath):\n",
    "    return [docDirPath + str(doc)+'.txt' for doc in classDocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classCountDict, classDocsDict = countDocsInClass(trainingDataLabel)\n",
    "totalDocsCount = countAllDocs(classCountDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run NaiveBayesClassifier.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbClassifier = NaiveBayesClassifier() # initialize classifier\n",
    "\n",
    "for classID in classDocsDict.keys(): # iterate each classID\n",
    "    priorProb = classCountDict[classID] / totalDocsCount # count prior prob of Naive Bayes\n",
    "    classDocs = classDocsDict[classID] # get docs list of classID\n",
    "    docPaths = getClassDocPaths(classDocs, docDirPath)\n",
    "    \n",
    "    # get term and term occur freq from docs in class. (k=term, v=freq)\n",
    "    termFreqOfDocs_dict = extractVocabularyFromDocs(docPaths)\n",
    "    \n",
    "    totalTokenCount = getTotalVocabularyCountFromDocs(termFreqOfDocs_dict) # how many vocabularys\n",
    "    diffTermCount = len(termFreqOfDocs_dict.keys()) # how many different terms\n",
    "    \n",
    "    condProb = dict()\n",
    "    for term, termOccurFreq in termFreqOfDocs_dict.items(): # get term occur freq (have been calculated)\n",
    "        condProb[term] = (termOccurFreq + 1) / ( totalTokenCount + diffTermCount)\n",
    "    \n",
    "    nbClassifier.updateClassPriorProbability(classID, priorProb) # update class prior prob\n",
    "    nbClassifier.updateClassConditionProbability(classID, condProb) # update class cond prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nbClassifier.getClassPriorProbability(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nbClassifier.getClassConditionProbability(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getLogScore(prob):\n",
    "#     print(prob)\n",
    "    return math.log(prob)\n",
    "\n",
    "def addScoreWithFrequency(scoreBuff, score, freq):\n",
    "    for i in range(freq):\n",
    "        scoreBuff += score\n",
    "    return scoreBuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateClassScoreRecord(classScoreList, classID, score):\n",
    "    classScoreList.append((classID, score))\n",
    "        \n",
    "def getResultClass(classScoreList):\n",
    "    classScoreList.sort(key=lambda s:s[1], reverse=True)\n",
    "    print(classScoreList)\n",
    "    maxRecord = classScoreList[0]\n",
    "    resultClass = maxRecord[0]\n",
    "    return resultClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docID = 5\n",
    "docPath = docDirPath + str(docID) + '.txt'\n",
    "docTermFreqDict = extractVocabularyFromSingleDoc(docPath)\n",
    "classes = nbClassifier.getAllClass()\n",
    "\n",
    "classScoreList = list()\n",
    "for c in classes:\n",
    "    scoreOfClass = 0\n",
    "    \n",
    "    priorProb = nbClassifier.getClassPriorProbability(c)\n",
    "    priorScore = getLogScore(priorProb) # get prior log score\n",
    "    \n",
    "    scoreOfClass = addScoreWithFrequency(scoreOfClass, priorScore, 1)\n",
    "    \n",
    "    condProb_dict = nbClassifier.getClassConditionProbability(c)\n",
    "    for term, occurFreq in docTermFreqDict.items():\n",
    "        if term not in condProb_dict.keys(): continue\n",
    "        condProb = condProb_dict[term]\n",
    "        condProbScore = getLogScore(condProb)\n",
    "        scoreOfClass = addScoreWithFrequency(scoreOfClass, condProbScore, occurFreq)\n",
    "        \n",
    "    updateClassScoreRecord(classScoreList, c, scoreOfClass) # update class score\n",
    "\n",
    "print(getResultClass(classScoreList))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
